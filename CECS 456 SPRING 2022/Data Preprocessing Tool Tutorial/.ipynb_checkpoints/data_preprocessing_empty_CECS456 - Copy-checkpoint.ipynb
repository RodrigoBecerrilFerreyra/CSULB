{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD-ZWwZZ5Y8y"
   },
   "source": [
    "##CECS 456-Dr. Wenlu Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools\n",
    "\n",
    "\n",
    "1.   In this lecture, it contains all the different tools of data preprocessing that you might have to use on your data sets in order to pre-process them the right way for your machine learning model.\n",
    "2.   data preprocessing: the first very important step of ML. Any time you build a ML model, you always have a data preprocessing phase to work on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # number and matrix processing\n",
    "from matplotlib import pyplot as plt # plotting library\n",
    "import pandas as pd # data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Data.csv\") # import CSV\n",
    "print(dataset)\n",
    "X = dataset.iloc[:, :-1].values # Country, Age, and Salary (Matrix)\n",
    "y = dataset.iloc[:,  -1].values # Purchased (Column vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44.0 72000.0]\n",
      " [27.0 48000.0]\n",
      " [30.0 54000.0]\n",
      " [38.0 61000.0]\n",
      " [40.0 nan]\n",
      " [35.0 58000.0]\n",
      " [nan 52000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:, 1:3]) # exclude Country and Purchased\n",
    "# note that X[:, 1:3] == X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Option 1: delete invalid entries from data.\n",
    "# Option 2: \"classic method\": replace missing data with average of all the values in a column\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") # fix data using mean\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "# Use one-hot encoding for categorical data\n",
    "# Let France = [1; 0; 0], Germany = [0; 1; 0], and Spain = [0; 1; 1]\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[(\"encoder\", OneHotEncoder(), [0])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# No = 0, Yes = 1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aNfI2Giikyt"
   },
   "source": [
    "Note: I will talk about applying k-Fold Cross Validation in the later class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nub18JX0fH6y"
   },
   "source": [
    "***most frequently asked quesions:***\n",
    "\n",
    "do we have to apply feature scaling before splitting the data into training set and teting set or after it?\n",
    "\n",
    "We apply feature scaling after the split. This is because our testing dataset would be biased if we applied feature scaling to it. Applying feature scaling before the split is called information leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " [[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "X_test:\n",
      " [[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "y_train:\n",
      " [0 1 0 0 1 1 0 1]\n",
      "y_test:\n",
      " [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# 80% in training dataset, 20% in testing (test_size)\n",
    "# random_state is the seed\n",
    "print(\"X_train:\\n\", X_train)\n",
    "print(\"X_test:\\n\", X_test)\n",
    "print(\"y_train:\\n\", y_train)\n",
    "print(\"y_test:\\n\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ArApiCY3gJn"
   },
   "source": [
    "Normalization (not preferred)\n",
    "`from sklearn.preprocessing import normalize`\n",
    "$$x' = \\frac{x - \\operatorname{min}(x)}{\\operatorname{max}(x) - \\operatorname{min}(x)}$$\n",
    "\n",
    "Standardization (preferred)\n",
    "`from sklearn.preprocessing import StandardScaler`\n",
    "$$x' = \\frac{x - \\bar{x}}{\\sigma} \\text{ where \\(\\bar{x}\\) is the mean of \\(x\\) and \\(\\sigma\\) is the standard deviation.} $$\n",
    "\n",
    "Normalization only works if data is already a normal distribution; standardization works all the time.\n",
    "\n",
    "you won't have to use feature scaling all the time, we will see, in each of the machine learning model implementation whether we have apply feature scaling or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train before:\n",
      " [[38.77777777777778 52000.0]\n",
      " [40.0 63777.77777777778]\n",
      " [44.0 72000.0]\n",
      " [38.0 61000.0]\n",
      " [27.0 48000.0]\n",
      " [48.0 79000.0]\n",
      " [50.0 83000.0]\n",
      " [35.0 58000.0]]\n",
      "X_test before:\n",
      " [[30.0 54000.0]\n",
      " [37.0 67000.0]]\n",
      "X_train after:\n",
      " [[-0.19159184384578545 -1.0781259408412425]\n",
      " [-0.014117293757057777 -0.07013167641635372]\n",
      " [0.566708506533324 0.633562432710455]\n",
      " [-0.30453019390224867 -0.30786617274297867]\n",
      " [-1.9018011447007988 -1.420463615551582]\n",
      " [1.1475343068237058 1.232653363453549]\n",
      " [1.4379472069688968 1.5749910381638885]\n",
      " [-0.7401495441200351 -0.5646194287757332]]\n",
      "X_test after:\n",
      " [[-1.4661817944830124 -0.9069571034860727]\n",
      " [-0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# We start from 3 so we can skip the one-hot encoding.\n",
    "# We skip it because it's categorical data: feature scaling would destroy the data.\n",
    "print(\"X_train before:\\n\", X_train[:, 3:])\n",
    "print(\"X_test before:\\n\", X_test[:, 3:])\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:]) # we only use transform to avoid training on the testing dataset\n",
    "print(\"X_train after:\\n\", X_train[:, 3:])\n",
    "print(\"X_test after:\\n\", X_test[:, 3:])\n",
    "\n",
    "# sc.fit calculates \\bar{x} and \\sigma; sc.transform applies the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV2U_NWAxd0X"
   },
   "source": [
    "**One of the most frequently asked questions in the data science community**\n",
    "\n",
    "\n",
    "> Do we have to apply feature scaling (standardization) to the dummy variables in the matrix of features?\n",
    "\n",
    "\n",
    "No.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_empty_CECS456.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
